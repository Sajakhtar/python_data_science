{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Overview\n",
    "\n",
    "* Explanaition of Hadoop, MapReduce, Spark and PySpark\n",
    "* Local versus Distributed Systems\n",
    "* Overview of Hadoop Ecosystem\n",
    "* Detailed overview of Spark\n",
    "* Setup on AWS\n",
    "* Resources on other Spark options\n",
    "* Jupyter Notebook hands-on code with PySpark and RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "* What can we do if we have large data set that a local computer can't handle\n",
    "    * Try using a SQL database to move storage onto hard drive instead of RAM\n",
    "    * Or use a distributed system, that distributes the data across multiple machines/ computers\n",
    "        * have one master node, controlling a network of distributed nodes\n",
    "        \n",
    "___\n",
    "\n",
    "* A local process will use the computation resources of a single machine\n",
    "* A distributed proces shas access to the computaitonal resources across a number of machines connected through a network\n",
    "* After a certain point, it is easier to scale out to many lower CPU machines, that to try to scale up a single machine with a higher CPU  \n",
    "\n",
    "___\n",
    "\n",
    "* Distributed machines also have the advantage of easily scaling, you can just add more machines\n",
    "* They also include fault tolerance, if on machine fails, the whole network can still go on\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "\n",
    "* Hadoop is a way to distribute very large files across multiple machines\n",
    "    * Name Node (CPU, RAM)\n",
    "        * Data (Child) Node (CPU, RAM)\n",
    "        * Data (Child) Node (CPU, RAM)\n",
    "        * Data (Child) Node (CPU, RAM)\n",
    "* It uses the Hadoop Distributed File System (**HDFS**)\n",
    "* HDFS allows a user to work with large data sets\n",
    "* HDFS also duplicates blocks of data for fault tolerance\n",
    "* It also uses **MapReduce**, which allows computations on that data\n",
    "    * Job Tracker (CPU, RAM)\n",
    "        * Task Tracker (CPU, RAM)\n",
    "        * Task Tracker (CPU, RAM)\n",
    "        * Task Tracker (CPU, RAM)\n",
    "\n",
    "___\n",
    "* HDFS will use blocks of data, with size of 128Mb by default\n",
    "* Each block of these is replicated 3 times\n",
    "* The blocks are distributed in a way to support fault tolerance\n",
    "* Smaller blocks provide more parallelization during processing\n",
    "* Multiple copies of a block prevent loss of data due to failure of a node      \n",
    "___\n",
    "* Map Reduce is a way of splitting a computation task to a distributed set of files (such as HDFS)\n",
    "* It consides of a Job Tracker and multiple Task Trackers\n",
    "* The Job Tracker sends code to run on the Task Trackers\n",
    "* The Task Trackers allocate CPU and memory for the tasks and monitor the task on the worker nodes\n",
    "___\n",
    "* Two distinct parts\n",
    "    1. HDFS to distribute large data sets\n",
    "    2. MapReduce to distribute a computational task to a distributed data set\n",
    "* Spark is the latest technology in this space and improves on the concept of using distribution\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "* Abstract overivew\n",
    "    * Spark\n",
    "    * Spark vs MapReduce\n",
    "    * RDD Operations\n",
    "    \n",
    "___\n",
    "\n",
    "* Spark is one of the latest technologies being used to quickly and easily handle Big Data\n",
    "* It's an open source project from Apache\n",
    "* First release in 2013 and has since exploded in popularity due to its ease of use and speed\n",
    "* It was created at the AMPLab at UC Berkeley\n",
    "* Think of Spark as a flexible alternative to MapReduce, not Hadoop\n",
    "* Spark can use data stored in a variety of formats\n",
    "    * AWS S3\n",
    "    * HDFS\n",
    "    * Cassandra\n",
    "    * and more...\n",
    "\n",
    "___\n",
    "\n",
    "* Spark vs MapReduce\n",
    "    * MapReduce requires files to be stored on HDFS, while Spark does not\n",
    "    * Spark can perform computation 100x faster than MapReduce\n",
    "    * How is Spark faster?\n",
    "        * MapReduce writes most data to disk after each map and reduce operation\n",
    "        * Spark keeps most of the data in memory after each transformation\n",
    "        * Spark can spill over to disk if memory is filled\n",
    "\n",
    "___\n",
    "\n",
    "* At the core of Spark is the idea of a Resilient Distributed Dataset (**RDD**)\n",
    "* **RDD** has 4 main features\n",
    "    1. Distributed collection of data\n",
    "    2. Fault-tolerant\n",
    "    3. SupportpParallel operation by partitioning the data\n",
    "    4. Ability to use many data sources\n",
    "___\n",
    "\n",
    "* Driver Program: SparkConext\n",
    "    * Cluster Manager\n",
    "        * Worker Node\n",
    "            * Executor\n",
    "                * Task\n",
    "                * Task\n",
    "            * Cache\n",
    "        * Worker Node\n",
    "            * Executor\n",
    "                * Task\n",
    "                * Task\n",
    "            * Cache\n",
    "            \n",
    "___\n",
    "* RDD Objects : build operator DAG (directed acyclic graph)\n",
    "    * DAGScheduler: splits graph into stages of tasks; submits each stage as ready\n",
    "        * TaskScheduler: launches tasks via cluster manager; retry failed/ straggling tasks\n",
    "            * Worker: executes tasks; stores + serves blocks\n",
    "            \n",
    "___\n",
    "\n",
    "* Spark allows developers to create complex multi-step data pipelines using DAG (directed acyclic graph) pattern\n",
    "* It also supports in-memory data sharing across the DAGs so different jobs can work with the same data\n",
    "\n",
    "___\n",
    "\n",
    "* RDDs are immutable, lazily evaluated, and cacheable\n",
    "* There are **2 types of RDD operations**\n",
    "    1. Transformations\n",
    "    2. Actions\n",
    "* We'll be coding Transformations and Actions, in python, on a distributed dataset\n",
    "* Basic Actions\n",
    "    * **First** - return the first element in the RDD\n",
    "    * **Collect** - return all the elements of the RDD as an array at the driver program\n",
    "    * **Count** - return the number of elements in the RDD\n",
    "    * **Take** - return an array with the first n elements of the RDD\n",
    "* Basic Transformations\n",
    "    * Filter - **RDD.filter()** applies a function to each element and returns elements that evaluate to true, similar to python built-in .filter()\n",
    "    * Map - **RDD.map()** transforms each element and preserves the same number of elements, similar to panads .apply()\n",
    "    * FlatMap - **RDD.flatMap()** transforms each element into 0-N elements and will probably change the number of elements to the original RDD\n",
    "    \n",
    "___\n",
    "\n",
    "Confustion between Map() and FlatMap(), examples to help clarify\n",
    "* Map() - grapping first leeter of a list of names\n",
    "* FlatMap() - transforming a corpus of text into a list of words\n",
    "\n",
    "___\n",
    "\n",
    "**Pair RDDs**\n",
    "* Often RDDs will be holding their values in tuples (key, value)\n",
    "* This offers better partitioning of data nad leads to functionality based on reduction\n",
    "\n",
    "**Reduction methods**\n",
    "* **Reduce()**\n",
    "    * An action that will aggregate RDD elements using a funciton that reutrns a single element \n",
    "* **ReduceByKey()**\n",
    "    * An action that will aggregate Pari RDD elements using a function that returns a pair RDD\n",
    "* These ideas are similar to a .groupby() operation\n",
    "___\n",
    "\n",
    "Spark is continually delveoped and new releases come out often\n",
    "Spark Ecosystem now includes\n",
    "    * Spark SQL\n",
    "    * Spark DataFrames\n",
    "    * MLib\n",
    "    * GraphX\n",
    "    * SparkStreaming\n",
    "    \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Setup\n",
    "\n",
    "* Sign up for free tier 12 months\n",
    "* Guide to creating AWS account: https://docs.aws.amazon.com/AmazonSimpleDB/latest/DeveloperGuide/AboutAWSAccounts.html\n",
    "* We'll be using Amazon Elastic Compute Cloud **EC2** \n",
    "    * think of EC2 as a Virtual Machine, in which we'll set everything up\n",
    "    * we'll be using the 't2.micro instance usage' 750 hrs per months free for 12 months \n",
    "    * https://aws.amazon.com/ec2/pricing/on-demand/\n",
    "        * t2.micro \\$0.0116 per Hour\n",
    "    * https://aws.amazon.com/ec2/spot/pricing/\n",
    "        * t2.micro \\$0.0081 per Hour\n",
    "* Make sure to shut resources down after use\n",
    "\n",
    "___\n",
    "\n",
    "**EC2 Instance and Spark  Setup**\n",
    "\n",
    "* **Create EC2 instance on AWS**\n",
    "    * AWS login > Services > EC2 > Launch instance\n",
    "        * select Ubuntu Free Tier Eligible > General Purpose t2.micro Free Tier Eligible\n",
    "        * Next: Configure Details >>\n",
    "            * Number of instances = 1 (default)\n",
    "            * leave default settings\n",
    "        * Next: Add Storage >>\n",
    "            * 8 GB (default)\n",
    "            * leave default settings\n",
    "        * Next: Add Tags >>\n",
    "            * Add\n",
    "                * key: myspark\n",
    "                * value: mymachine\n",
    "        * Next: Configure Security Group >>\n",
    "            * Type: All traffic (default is SSH)\n",
    "            * leave everything else to default settings\n",
    "            **NOTE:** When we set up the EC2 instance and configure the security groups setting for these spot instances we keep all the ports are open for simplicity, but it should be noted that these settings should be much more strict if put in production.\n",
    "        * Review and Launch >>\n",
    "        * Launch >>\n",
    "            * Create a new key pair\n",
    "                * key pair name: newspark\n",
    "                * Download Key Pair >>\n",
    "                    * privae key file (.pem)\n",
    "                    * *Won't be able to download file again after this window, so make sure to download it*\n",
    "        * Launch Instances >> / Request Spot Instances >>\n",
    "        * Click on the instance number/ID\n",
    "            * now we have an Instance ID and a public DNS (IP address) and .pem file for our VM \n",
    "            * perform actions\n",
    "                * Actions > Instance State > Stop/ Reboot / Terminate\n",
    "                * we'll use Terminate once we're done    \n",
    "* **Use SSH to conenct to EC2 over internet**\n",
    "    * SSH is different for Windows vs Mac/Linux\n",
    "        * our goal is to remotely connect to the commmand line of our virtual machine running on EC22\n",
    "        * Windows: \n",
    "            * follow these steps: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html\n",
    "            * Download PuTTy: https://www.chiark.greenend.org.uk/~sgtatham/putty/\n",
    "                * putty.exe\n",
    "                * puttygen.exe\n",
    "            * need EC2 instance ID, publicis DNS name from AWS EC2 UI\n",
    "            * and need .pem file\n",
    "            * convert .pem file to .ppk file usig PuTTYGen\n",
    "                * open **puttygen.exe**\n",
    "                * Type of Key: RSA or SSH-2 RSA\n",
    "                * Load >> the .pem file\n",
    "                * Save private key >>\n",
    "                    * save to folder with file name puttyspark.ppk\n",
    "            * start PuTTY session\n",
    "                * click **putty.exe**\n",
    "                * Session pane (default)\n",
    "                    * Hostname: prefix 'ubuntu@' to Public DNS name from EC2 Instance UI\n",
    "                * Connection pane\n",
    "                    * SSH > Auth\n",
    "                        * Browse: .ppk file\n",
    "                * open >> to start putty session\n",
    "                    * say yes to security warning\n",
    "                * now we have a terminal connected to our EC2 Ubunu VM\n",
    "                    * can run python etc...\n",
    "        * Mac/Linux: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html\n",
    "            * simpler on Max/ Linux as SSH is built in and only require 2 simple command lines\n",
    "        * Secure Shell Connection (SSH)\n",
    "            * start PuTTY session\n",
    "                * click **putty.exe**\n",
    "                * Session pane (default)\n",
    "                    * Hostname: prefix 'ubuntu@' to Public DNS name from EC2 Instance UI\n",
    "                * Connection pane\n",
    "                    * SSH > Auth\n",
    "                        * Browse: .ppk file\n",
    "                * open >> to start putty session\n",
    "                    * say yes to security warning\n",
    "                * now we have a terminal connected to our EC2 Ubunu VM\n",
    "                    * can run python etc...\n",
    "                    * `python3`\n",
    "* **Setup Spark and Jupyter on EC2 instance**\n",
    "    * https://medium.com/@josemarcialportilla/getting-spark-python-and-jupyter-notebook-running-on-amazon-ec2-dec599e1c297\n",
    "    * install Anaconda\n",
    "        * `wget http://repo.continuum.io/archive/Anaconda3-4.1.1-Linux-x86_64.sh`\n",
    "            * can change to the latest version of python 2 or 3\n",
    "        * `bash Anaconda3–4.1.1-Linux-x86_64.sh`\n",
    "            * enter to go through license\n",
    "            * 'yes' to continue\n",
    "            * 'yes' to installer location\n",
    "    * Check installation by seeing with python we're using (anaconda vs ubuntu built-in)\n",
    "        * `which python`\n",
    "    * change to anaconda python\n",
    "        * `source .bashrc`\n",
    "            * should say `/home/ubuntu/anaconda3/bin/python`\n",
    "        * now we can use `python` in the terminal\n",
    "    * config file for jupyter\n",
    "        * `jupyter notebook --generate-config`\n",
    "    * create certifications for our connections\n",
    "        * `mkdir certs`\n",
    "        * `cd certs`\n",
    "        * `sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem`\n",
    "            * `US`\n",
    "            * `CA`\n",
    "            * `San Jose`\n",
    "            * `Pieran Data`\n",
    "            * `Data`\n",
    "            * `Jose`\n",
    "            * email leave blank\n",
    "        * Once in the certs directory, change the permissions on the .pem file with: `sudo chmod 777 mycert.pem`\n",
    "    * Edit myrcert.pem\n",
    "        * `cd ~/.jupyter/`\n",
    "        * `vi jupyter_notebook_config.py`\n",
    "            * press `i` key for inset mode\n",
    "                * `c = get_config()`\n",
    "                * `# Notebook config this is where you saved your pem cert`\n",
    "                * `c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem'`\n",
    "                * `# Run on all IP addresses of your instance`\n",
    "                * `c.NotebookApp.ip = '*'`\n",
    "                * `# Don't open browser by default`\n",
    "                * `c.NotebookApp.open_browser = False`  \n",
    "                * `# Fix port to 8888`\n",
    "                * `c.NotebookApp.port = 8888`\n",
    "            * press `esc` to exit insert mode\n",
    "            * press `:wq!` then `enter`      \n",
    "    * Check Jupyter notebook is working\n",
    "       * `cd` back to home directory\n",
    "       * `jupyter notebook`\n",
    "           * should see the jupyter notebook is running on port 8888\n",
    "       * in chrome browser tab:\n",
    "           * navigtate to `https://` + `EC2 Publics DNS addres` + `:8888`\n",
    "           * click through untrusted certificate warnings\n",
    "       * jupyter notebook instance, connected to the EC2 VM, should appear\n",
    "       * kill the notebook using `ctrl + c`\n",
    "    * Install Spark\n",
    "        * need to first install Java and Scala\n",
    "            * `sudo apt-get update`\n",
    "            * `sudo apt-get install default-jre\n",
    "            * `java -version`\n",
    "            * `sudo apt-get install scala`\n",
    "            * `scala -version`\n",
    "        * Install Py4j\n",
    "            * `export PATH=$PATH:$HOME/anaconda3/bin`\n",
    "            * `conda install pip`\n",
    "            * `which pip`\n",
    "                * should say `/home/ubuntu/anaconda3/bin/pip`\n",
    "            * `pip install py4j`\n",
    "        * Install Spark and Hadoop\n",
    "            * `wget http://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz`\n",
    "            * `sudo tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz`\n",
    "        * Tell python where to find spark\n",
    "            * `export SPARK_HOME='/home/ubuntu/spark-2.0.0-bin-hadoop2.7'`\n",
    "            * `export PATH=$SPARK_HOME:$PATH`\n",
    "            * `export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH`\n",
    "        * Launch jupyter notebook `jupyter notebook`\n",
    "            * in browser navigtate to `https://` + `EC2 Publics DNS addres` + `:8888`\n",
    "            * create a new notebook\n",
    "                * `from pyspark import SparkContext`\n",
    "                * `sc = SparkContext()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
